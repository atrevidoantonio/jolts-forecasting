<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>JOLTS.knit</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="JOLTS.html">Technical Stuff</a>
</li>
<li>
  <a href="welcome.html">Overview</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Estimation of the job finding probability comes from a line of research in the so-called “Search-Theoretic” approach to unemployment and duration analysis . Duration analysis has its origins in survival analysis, a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems . It involves the modeling of time to event data; in our context, finding a job or becoming unemployed is considered an “event.” Since in the social sciences, we are interested in any situation where an individual or family, or firm, and so on begins in an initial state and is either observed to exit the state or is censored.</p>
<p>Duration analysis typically begins by specifying a population distribution for the duration, usually conditional on some explanatory variables observed at the beginning of the duration. For example, for the population of people who became unemployed during a particular period, we might observe education levels, experience, marital status-all measured when the person becomes unemployed-wage on prior job, and a measure of unemployment benefits. Then we specify a distribution for the unemployment duration conditional on the explanatory variables. Any reasonable distribution reflects the fact that an unemployment duration is non-negative. Once a complete conditional distribution has been specified, maximum likelihood methods are used to estimate the expected duration.</p>
<p>The hazard function allows us to approximate the probability of exiting the initial state within a short interval, conditional on having survived up to the starting time of the interval. In econometric applications, hazard functions are usually conditional on some covariates.</p>
<p>The survival function <span class="math inline">\(S(t)\)</span>, is the probability that a subject survives or in this case endures longer than time <span class="math inline">\(t\)</span>. If time is continuous with a cumulative distribution function (cdf) <span class="math inline">\(F(t)\)</span> then the survival function is <span class="math display">\[\begin{equation}
\begin{gathered}
S(t) = \mathrm{P}(\{T &gt; t\}) = \int\limits_{t}^{\infty} f(u)\, du \\[8pt]
= 1 - F(t)
\end{gathered}
\end{equation}\]</span> and this is the probability of “surviving” past time <span class="math inline">\(t\)</span>. The density of <span class="math inline">\(T\)</span> can be denoted <span class="math display">\[
f(t) = \frac{\partial F}{\partial t}(t)
\]</span> The probability that a person who has occupied a state for time <span class="math inline">\(t\)</span> leaves it in some short interval <span class="math inline">\(h\)</span> after <span class="math inline">\(t\)</span> is given by, <span class="math display">\[
\mathrm{P}(t \leq T &lt; t + h \; | \; T \geq t)
\]</span> The hazard or “exit” function is then, <span class="math display">\[\begin{equation}
\lambda(t) = \lim_{h \;|\, 0} \frac{\mathrm{P}(t \leq T &lt; t + h \; | \; T \geq t)}{h}
\end{equation}\]</span></p>
<p>For each <span class="math inline">\(t\)</span>, <span class="math inline">\(\lambda(t)\)</span> is the instantaneous rate of leaving per unit of time.</p>
<p>A rough interpretation of <span class="math inline">\(\lambda(t)\)</span> is that it presents the probability of exit from a state in the short interval <span class="math inline">\(h\)</span> after <span class="math inline">\(t\)</span>, conditional on the state being occupied at <span class="math inline">\(t\)</span>. Put another way, the hazard function gives the probability that a person unemployed for 10 weeks (<span class="math inline">\(\lambda(10)\)</span>) will find employment (exit unemployment), whereas the unconditional probability provides the chances that a person will get job at the tenth week of unemployment. That is <span class="math inline">\(\lambda(10)\)</span> gives the probability that a person will find a job between weeks 10 and 11, conditional on being unemployed through week 10.</p>
<p>We can express the hazard function and the unconditional probability of exit in terms of the distribution and the probability density function (pdf) of <span class="math inline">\(T\)</span>, which is considered to be a continuous random variable. By the law of conditional probability,</p>
<p><span class="math display">\[\begin{equation}
\begin{gathered}
\frac{\mathrm{P}(t \leq T &lt; t + h \; | \; T \geq t)}{\mathrm{P}(T \geq t)} \\[8pt]
= \frac{F(t + h) - F(t)}{1 - F(t)}
\end{gathered}
\end{equation}\]</span></p>
<p>When the cdf is differentiable, we can take the limit of the right-hand side, divided by <span class="math inline">\(h\)</span>, as <span class="math inline">\(h\)</span> approaches zero:</p>
<p><span class="math display">\[\begin{equation}
\begin{gathered}
\lambda(t) = \lim_{h \to 0} \frac{F(t + h) - F(t)}{h}\cdot \frac{1}{1 - F(t)} \\[8pt]
= \frac{\partial F}{\partial t}F(t) \cdot \frac{1}{1 - F(t)} \\[8pt]
= \frac{f(t)}{1 - F(t)} \\[8pt]
= \frac{f(t)}{S(t)}
\end{gathered}
\end{equation}\]</span></p>
<p>Because the first derivative of <span class="math inline">\(S(t)\)</span> is <span class="math inline">\(-f(t)\)</span> we can also express the hazard function as</p>
<p><span class="math display">\[\begin{equation}
\lambda(t) = - \frac{\partial \ln S(t)}{\partial t}
\end{equation}\]</span></p>
<p>Equation (5) is a differential equation in <span class="math inline">\(t\)</span>, whose solution, subject to the initial condition <span class="math inline">\(F(0) = 0\)</span> we arrive through integration</p>
<p><span class="math display">\[\begin{equation}
F(t) = 1 - \exp\left[\int\limits_{0}^{t}\,\lambda(s)\,ds \right], \quad t \geq 0
\end{equation}\]</span></p>
<p>differentiation of equation (6) gives the density of <span class="math inline">\(T\)</span> as</p>
<p><span class="math display">\[\begin{equation}
f(t) = \lambda(t)\exp\left[\int\limits_{0}^{t}\,\lambda(s)\,ds \right],
\end{equation}\]</span></p>
<p>The shape of the hazard function is important in many empirical aspects and in the simplest case it is constant <span class="math inline">\(\lambda(t) = \lambda\)</span>. This function means that the process driving <span class="math inline">\(T\)</span> is memoryless: the probability of exit in the next interval does not depend on how much time has been spent in the initial state. A constant hazard implies</p>
<p><span class="math display">\[
F(t) = 1 - e^{-\lambda t}
\]</span></p>
<p>When the hazard function is not constant, the process exhibits duration dependence . Assuming that <span class="math inline">\(\lambda(t)\)</span> is differentiable, there is positive duration dependence at time <span class="math inline">\(t\)</span> if <span class="math inline">\(\partial\lambda(t)/\partial t &gt; 0\)</span>, then the process exhibits positive duration dependence. Positive duration dependence means simply that the probability of exiting the initial state increases the longer one is in the initial state. If the derivative is negative, however, then there is negative duration dependence; longer spells in the initial decreases the probability of exit.</p>
<p> argue that movements in the job finding probability accounted for about 65 percent of unemployment fluctuations prior to the last two recessions, and more in 1990–1991 and 2001.  claim a more substantial role for the hazard rate to unemployment (separation rate) but still find that the job finding probability accounts for at least half of the fluctuations in unemployment.</p>
<p>Following the approach of , we can roughly approximate the hazard rate (job finding probability) as</p>
<p><span class="math display">\[\begin{equation}
F_{t} = 1 - \frac{u_{t} - s_{t}}{u_{t-1}}
\end{equation}\]</span></p>
<p>which expresses the monthly outflow finding probability as a function of the total unemployment level <span class="math inline">\(u_{t}\)</span> at time <span class="math inline">\(t\)</span> and short term unemployment <span class="math inline">\(s_{t}\)</span>, defined as those unemployed for less than five weeks.</p>
<p>Shimer’s method extended by  shows that the monthly outflow hazard rate, <span class="math inline">\(f(t)\)</span> can be transformed from (8) into a continuous-time outflow hazard as,</p>
<p><span class="math display">\[\begin{equation}
f_{t} = - \ln(1 - F_{t})
\end{equation}\]</span></p>
<p>Calculating this measure from BLS aggregate unemployment duration flows allows us to look at a longer view of the job finding probability.</p>
<p><img src="JOLTS_files/figure-html/fig6-1.png" width="4060" style="display: block; margin: auto;" /></p>
<p> argues that fluctuations in the unemployment rate are mostly explained by fluctuations in job finding rates or transitions out of unemployment (“Outs”) rather than by fluctuations in job separation rates or transition into unemployment (“Ins”). This implies that the unemployment rate fluctuates over the business cycle not because workers lose their jobs but because workers have a hard time finding a job.</p>
<p>The declining job finding probability is worrying for a number of reasons. A large literature documents that a particular type of long-term unemployment - those resulting from job losses during mass layoffs - leave a very persistent “scarring” effect on the future earnings of displaced workers. Future earnings of people who experienced prolonged spells of unemployment decline by as much as 20 percent . A more recent study by  finds that the loss is potentially larger at around 35-40 percent. Furthermore, due to negative duration dependence, the probability of receiving a callback for an interview significantly decreases with the length of a worker’s unemployment spell, with the majority of this decline occurring during the first eight months . In short, what economists call “negative duration dependence” undermines the functioning of the labor market and generates large social costs.</p>
<p>The unusual nature of the COVID-19 recession makes it difficult to draw on experiences from past recessions to project how the labor market will evolve in the months ahead. For example, during the Great Recession, the initial wave of layoffs was subsequently followed by a prolonged period of lower job finding rates . This led to a significant increase in the long-term unemployment share, which in turn prolonged the recession through negative duration dependence . Currently available data suggest that the dynamics of the COVID-19 recession may play out quite differently. Looking more closely at the last two decades in Figure 8 we see that job finding rates have not decreased as substantially as they did in the Great Recession.</p>
<p>The pandemic is an urgent reminder of what long-term labor trends have been illustrating for years: low-wage workers need better pathways into decent jobs, and from shrinking occupations to the jobs of tomorrow. Policymakers face a dual imperative: to facilitate safe reemployment as soon as possible, even as COVID-19 continues to surge in many parts of the country, while also helping low-wage workers on the journey to jobs with dignity, stability, and a fair shot at economic mobility. While the risk of mass unemployment has already spurred large public expenditures, more funding and efforts are needed to ensure opportunity reaches those who need it the most.</p>
</div>
<div id="methods" class="section level2">
<h2>Methods</h2>
<p>We produce forecasts using a collection of traditional and non-traditional time series methods. This section provides a general overview of the methods used, their benefits, and their limitations. It cannot be emphasized enough that, no matter the strength of a model, it remains exactly that: a model. As such, all statistical models are “wrong.” No matter the method used, any model is an attempt to reproduce (“model”) the true data generating process of a data series.</p>
</div>
<div id="theorectical-background" class="section level2">
<h2>Theorectical Background</h2>
<p>For many familiar with the methodology of time series forecasting, this section presents a brief recapitulation of basic concepts which can be found in any standard textbook such as  or . For a more advanced treatment, one should see  or .</p>
<p>Time series analysis is the procedure of using known data values to fit a time series with a suitable model and estimation of the corresponding parameters. It comprises methods that attempt to understand the nature of the time series.</p>
<p>A major assumption that often provides relief in modeling efforts is the linearity assumption. A linear filter, for example, is a linear operation from one time series <span class="math inline">\(x_{t}\)</span> to another time series <span class="math inline">\(y_{t}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
E[y_{t}] = L(x_{t}) = A(L^{i})\epsilon_{t} = \left(\sum\limits_{-\infty}^{\infty} \psi_{i}L^{i}\right)\epsilon_{t} = \mu + \sum\limits_{-\infty}^{\infty}\psi_{i}\epsilon_{t-i},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^{2})\)</span> and <span class="math inline">\(L^{i}\)</span> is the Lag or Backshift Operator defined as,</p>
<p><span class="math display">\[
L^{i}x_{t} = x_{t-i} \; \forall \, i \in \mathbb{N}
\]</span></p>
<p>the linear filter can be seen as a process that converts the input, <span class="math inline">\(x_{t}\)</span>, into an output, <span class="math inline">\(y_{t}\)</span> with a conversion process that involves all (present, past, and future) values of the input in the form of a summation with different “weights,” <span class="math inline">\(\psi_{i}\)</span>, on each observation <span class="math inline">\(x_{i}\)</span>. Specifically, <span class="math inline">\(y_{t}\)</span> can be expressed as</p>
<p><span class="math display">\[
y_{t} = \mu + \sum\limits_{i= o}^{\infty}\psi_{i}\epsilon_{t-i}
\]</span></p>
<p>where the weights are conventionally some form of moving average <span class="math inline">\(M\)</span>. A moving average <span class="math inline">\(M\)</span> of span <span class="math inline">\(N\)</span> assigns weights <span class="math inline">\(\frac{1}{N}\)</span> to the most recent observations such that the estimate can be written as</p>
<p><span class="math display">\[
M_{t} = \frac{1}{N} \sum\limits_{t = T - N - 1}^{N} y_{t}
\]</span></p>
<p>The covariance between <span class="math inline">\(y_{t}\)</span> and its value at another point in time <span class="math inline">\(y_{t + k}\)</span> is called the auto-covariance at lag <span class="math inline">\(k\)</span>, and is defined by</p>
<p><span class="math display">\[
\gamma_{k} = \mathrm{Cov}(y_{t}, y_{t+k}) = E\left[(y_{t} - \mu)(y_{t+k} - \mu)\right]
\]</span></p>
<p>the autocovariance of the series at lag <span class="math inline">\(k = 0\)</span> is simply the variance of the time series. The autocorrelation coefficient at <span class="math inline">\(k\)</span> is then</p>
<p><span class="math display">\[\begin{gather*}
\rho_{k} = \frac{E[(y_{t} - \mu)(y_{t+k} -\mu)]}{\sqrt{E[(y_{t} - \mu)^{2}]E[(y_{t+k} - \mu)^{2}]}} \\[8pt]
= \frac{\mathrm{Cov}(y_{t}, y_{t+k})}{\mathrm{Var}(y_{t})}\\[8pt]
= \frac{\gamma_{k}}{\gamma_{0}}
\end{gather*}\]</span></p>
<p>The collection of these values <span class="math inline">\(\rho_{k}\)</span> is called the autocorrelation function. For a finite time series, it is necessary to estimate the autocovariance and autocorrelation functions. A usual estimate of the autorcovariance function is</p>
<p><span class="math display">\[
c_{k} = \hat{\gamma_{k}} = \frac{1}{T} \sum\limits_{t = 1}^{T-k}(y_{t} -\bar{y})(y_{t+k} - \bar{y}), \quad k = 1, 2, 3,\, \ldots,\, K
\]</span> and the autocorrelation function is estimated by the sample autocorrelation function (sample ACF) <span class="math display">\[
r_{k} = \hat{\rho_{k}} = \frac{c_{k}}{c_{o}}
\]</span> The stationarity of a time series is related to its statistical properties in time. That is, in the more strict sense, a stationary time series exhibits similar “statistical behavior” in time and this is often characterized as a constant probability distribution in time. However, it is usually satisfactory to consider the first two moments of the time series and define stationarity (or weak stationarity) as follows: (1) the expected value of the time series does not depend on time and (2) the autocovariance function defined as <span class="math inline">\(\mathrm{Cov}(y_{t}, y_{t+k})\)</span> for any lag <span class="math inline">\(k\)</span> is only a function of <span class="math inline">\(k\)</span> and not time: that is, <span class="math inline">\(y(k) = \mathrm{Cov}(y_{t}, y_{t+k})\)</span>. Weak stationarity, then, implies that the mean of the series does not change with time <span class="math inline">\(t\)</span> and that the autocovariance function, <span class="math inline">\(\gamma(k, t)\)</span>, depends on <span class="math inline">\(k\)</span> and <span class="math inline">\(t\)</span> only through their difference <span class="math inline">\(k - t\)</span>.</p>
<p>For a time-invariant and stable linear filter a stationary input time series <span class="math inline">\(y(t)\)</span> can be written more succinctly as,</p>
<p><span class="math display">\[\begin{gather*}
y(t) = \mu + \sum\limits_{i= 0}^{\infty}\psi_{i}\epsilon_{t} \\[8pt]
\mu + \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}\epsilon_{t} \\[8pt]
\mu + \Psi(L)\epsilon_{t}
\end{gather*}\]</span></p>
<p>where <span class="math display">\[
  \Psi(L)\epsilon_{t} = \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}
  \]</span><br />
The infinite moving average model has a covariance function in the form,</p>
<p><span class="math display">\[
\mathrm{Cov}(y_{t}, y_{t+k}) = \gamma_{y}(k, t) = \sum\limits_{i = -\infty}^{\infty}\sum\limits_{j = - \infty}^{\infty}\psi_{i}\psi_{j}\gamma_{x}(i - j + k)
\]</span></p>
<p>The autocovariance function of <span class="math inline">\(y_{t}\)</span> is then,</p>
<p><span class="math display">\[\begin{gather*}
\gamma_{y}(k, t) = \sum\limits_{i = o}^{\infty}\sum^{\infty}\limits_{j = 0} \psi_{i}\psi_{j}\gamma_{\epsilon}(i-j+k) \\[8pt]
= \sigma^{2}\sum\limits_{i = 0}^{\infty}\psi_{i}\psi_{i + k}
\end{gather*}\]</span></p>
<p>This representation comes from a theorem by Wold and which essentially states that any nondeterministic weakly stationary time series <span class="math inline">\(y_{t}\)</span> can be represented as in the above manner. A more intuitive interpretation of this theorem is that a stationary time series can be seen as the weighted sum of the present and past random “disturbances.”</p>
<p>Although very powerful in providing a general representation of any stationary time series, the infinite moving average model given in is not very useful in practice except for certain special cases.</p>
<p>In finite order moving average or MA models, weights that are not set to 0 are represented by the Greek letter <span class="math inline">\(\theta\)</span> with a minus sign in front.</p>
<p><span class="math display">\[\begin{gather*}
y_{t} = \mu + \left(1 - \sum\limits_{i=1}^{q}\theta_{i}L^{i} \right)\epsilon_{i}\\[8pt]
= \mu + \Theta(L)\epsilon_{t}
\end{gather*}\]</span></p>
<p>where <span class="math inline">\(\Theta(L)\epsilon_{t} = 1 - \sum\limits_{i=1}^{q}\theta_{i}L^{i}\)</span></p>
<p>An interpretation of the finite order MA processes is that at any given time, of the infinitely many past disturbances, only a finite number of the disturbances “contribute” to the current value of the time series and that the time window of the contributors “moves” in time, making the “oldest” disturbance obsolete for the next observation. It is indeed not too far fetched to think that some processes might have these intrinsic dynamics. However, for some others, we may be required to consider the “lingering” contributions of the disturbances that happened back in the past. This will of course bring us back to square one in terms of our efforts in estimating infinitely many weights. Another solution to this problem is through autoregressive models in which the infinitely many weights are assumed to follow a distinct pattern and can be successfully represented with only a handful of parameters. We shall now consider some special cases of autoregressive processes.</p>
</div>
<div id="arma" class="section level2">
<h2>ARMA</h2>
<p>Autoregressive models are based on the idea that current value of the series, <span class="math inline">\(x_{t}\)</span> can be explained as a linear combination of past values, <span class="math inline">\(x_{t-1}, x_{t-2}, \ldots, \, x_{t-p}\)</span>, together with some random error <span class="math inline">\(\epsilon\)</span>. In other words, a series <span class="math inline">\(x_{t}\)</span> is linearly dependent upon its past values, the degree to which depends on the lag order <span class="math inline">\(p\)</span>.</p>
<p>Let us first consider again the time series</p>
<p><span class="math display">\[\begin{gather*}
y(t) = \mu + \sum\limits_{i= 0}^{\infty}\psi_{i}\epsilon_{t} \\[8pt]
\mu + \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}\epsilon_{t} \\[8pt]
\mu + \Psi(L)\epsilon_{t}
\end{gather*}\]</span></p>
<p>One approach to modeling this time series is to assume that the contributions of the disturbances that are way in the past should be small compared to the more recent disturbances that the process has experienced. Since the disturbances are independently and identically distributed random variables, we can simply assume a set of infinitely many weights in descending magnitudes reflecting the diminishing magnitudes of contributions of the disturbances in the past.</p>
<p>A simple and yet intuitive set of such weights can be created following an exponential decay pattern. For that we will set <span class="math inline">\(\psi_{i} = \phi^{i}\)</span>, where <span class="math inline">\(\left|\,\phi\,\right| &lt; 1\)</span> to guarantee the exponential “decay.” In this notation, the weights on the disturbances starting from the current disturbance and going back in past will be <span class="math inline">\(1, \phi^{2}, \phi^{3},\, \ldots\)</span></p>
<p><span class="math display">\[\begin{gather*}
y_{t} = \mu + \epsilon_{t} + \phi\epsilon_{t-1} + \phi^{2}\epsilon_{t-2} + \cdots\\[8pt]
= \mu + \sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i}
\\[8pt]
\end{gather*}\]</span></p>
<p>We also have</p>
<p><span class="math display">\[\begin{gather*}
y_{t-1} = \mu + \epsilon_{t-1} + \phi\epsilon_{t-2} + \phi^{2}\epsilon_{t-3} + \cdots \\[8pt]
= \mu + \phi\left(\sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-1-k} + \epsilon_{t}\right)
\end{gather*}\]</span></p>
<p>combining these two equations together we can represent an AR(1) model as a linear process given by</p>
<p><span class="math display">\[\begin{gather*}
\mu + \sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i} = \mu + \phi\left(\sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i} + \epsilon_{t}\right) \\[8pt]
\mu + \epsilon_{t} + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t-i} + \phi\mu \\[8pt]
\mu - \phi\mu + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t-i} + \epsilon_{t} \\[8pt]
\delta + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t- i} + \epsilon_{t}
\end{gather*}\]</span></p>
<p>where <span class="math inline">\(\delta = (1 - \phi)\mu\)</span>. A more general way of writing an autoregressive procress is through operator notation as</p>
<p><span class="math display">\[\begin{gather*}
\epsilon_{t} = \left(1 - \sum\limits_{i=1}^{i}\phi_{i}L^{i} \right)y_{t} \\[8pt]
 = \Phi(L)y_{t}
\end{gather*}\]</span></p>
<p>where <span class="math inline">\(\Phi(L) = 1 - \sum\limits_{i=1}^{p}\phi_{i}L^{i}\)</span></p>
<p>The above process in is called a first-order autoregressive process, AR(l), because it can be seen as a regression of <span class="math inline">\(y_{t}\)</span> on <span class="math inline">\(y_{t-1}\)</span> and hence the term autoregressive process.</p>
<p>Autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA). The general ARMA model was described in the 1951 dissertation of Peter Whittle, Hypothesis Testing in Time Series, which generalized Wold’s autoregressive representation theorem for univariate stationary processes to multivariate processes, and was popularized in by Box and Jenkins in their classic book .</p>
<p><span class="math display">\[\begin{gather*}
y_{t} = \delta + \sum\limits_{i=0}^{p}\phi^{i}y_{t- i} + \epsilon_{t} - \sum\limits_{i=1}^{q}\theta_{i}\epsilon_{i-1} \\[8pt]
\Psi(L)y_{t} = \delta + \Theta(L)\epsilon_{t} 
\end{gather*}\]</span></p>
<p>which has an infinite moving average process <span class="math inline">\(\mathrm{MA}(q)\)</span></p>
<p><span class="math display">\[
y_{t} = \mu + \sum\limits_{i= o}^{\infty}\psi_{i}\epsilon_{t-i}
\]</span></p>
</div>
<div id="arima" class="section level2">
<h2>ARIMA</h2>
<p>The Autoregressive Integrated Moving Average (ARIMA) model is a generalization of an Autoregressive Moving Average (ARMA) model to overcome the possible violation of the assumption that the series <span class="math inline">\(x_{t}\)</span> is stationary. In order for inferences drawn from the estimates to be reliable when the data are not stationary, the data must be differenced some number of times <span class="math inline">\(d\)</span> to remove seasonality and trend. If a stochastic process has to be differenced <span class="math inline">\(d\)</span> times to reach stationarity, it is said to be integrated of order <span class="math inline">\(d\)</span> or <span class="math inline">\(I(d)\)</span>.</p>
<p>A standard convention for expressing the ARIMA model is <span class="math inline">\(\mathrm{ARIMA}(p, d, q)\)</span>, where:</p>
<ul>
<li><p><span class="math inline">\(p\)</span> denotes the number of lag observations included in the model, also called the lag order.</p></li>
<li><p><span class="math inline">\(d\)</span> denotes the number of times that the series is differenced, also called the degree of differencing.</p></li>
<li><p><span class="math inline">\(q\)</span> denotes the size of the moving average window.</p></li>
</ul>
<p>We will call a time series homogeneous, non-stationary if it is not stationary but its first difference, that is, <span class="math inline">\(w_{t} = y_{t} - Y_{t-1} =(1 - L)y_{t}\)</span>,or higher-order differences, <span class="math inline">\(w_{t} =(I- L)^{d}y_{t}\)</span> produce a stationary time series. We will further call <span class="math inline">\(y_{t}\)</span> an autoregressive integrated moving average (<span class="math inline">\(\mathrm{ARIMA}\)</span>) process of orders <span class="math inline">\(p\)</span>, <span class="math inline">\(d\)</span>, and <span class="math inline">\(q\)</span> <span class="math inline">\(\mathrm{ARIMA}(p, d, q)\)</span> if its <span class="math inline">\(d\)</span>-th difference, denoted by <span class="math inline">\(w_{t} =(I- L)^{d}y_{t}\)</span> produces a stationary <span class="math inline">\(\mathrm{ARMA}(p, q)\)</span> process. The term integrated is used since, <span class="math inline">\(d = 1\)</span>, for example, we can write <span class="math inline">\(y_{t}\)</span> as the sum (or “integral”) of the <span class="math inline">\(w_{t}\)</span> process as</p>
<p><span class="math display">\[\begin{gather*}
y_{t} = \delta + \sum\limits_{i=0}^{p}\phi^{i}y_{t- i} + \epsilon_{t} - \sum\limits_{i=1}^{q}\theta_{i}\epsilon_{i-1} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right)y_{t} = \left(1 + \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\epsilon_{t} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right) = \left(1 + \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\left(1 - L\right)^{d} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right)\left(1 - L\right)^{d}y_{t} = \left(1 + \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\epsilon_{t} \\[8pt]
\Phi(L)(1 - L)^{d}y_{t} = \delta + \Theta(L)\epsilon_{t}
\end{gather*}\]</span></p>
<p>Selection of the hyperparameters <span class="math inline">\((p, d, q)\)</span> are often chosen through an inspection of the ACF and PACF, but can also be selected through AIC (Akaike Information Criterion), AICc (corrected AIC) and BIC (Bayesian Information Criterion). But note that the selection of the hyperparameters is not unique.</p>
</div>
<div id="svm" class="section level2">
<h2>SVM</h2>
</div>
<div id="rnn" class="section level2">
<h2>RNN</h2>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
</div>
<div id="unit-root" class="section level2">
<h2>Unit Root</h2>
<p>A unit root is a feature of some stochastic processes (such as random walks) that can cause problems in statistical inference involving time series models. A linear stochastic process has a unit root if 1 is a root of the process’s characteristic equation. Shocks to a unit root process have permanent effects which do not decay as they would if the process were stationary. The characteristic roots (roots of the characteristic equation) also provide qualitative information about the behavior of the variable whose evolution is described by the dynamic equation. For a differential equation parameterized on time, the variable’s evolution is stable if and only if the real part of each root is negative. For difference equations, such as a standard time series, there is stability if and only if the absolute value of each root is less than 1.</p>
<p>An augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.</p>
<p>The intuition behind the test is that if the series is characterised by a unit root process then the lagged level of the series (<span class="math inline">\(y_{t-1}\)</span>) will provide no relevant information in predicting the change in <span class="math inline">\(y_{t}\)</span> besides the one obtained in the lagged changes. The OLS estimate (based on an <span class="math inline">\(n\)</span>-observation time series) of the autocorrelation parameter <span class="math inline">\(\rho\)</span> is given by</p>
<p><span class="math display">\[
\hat{\rho}(n) = \frac{\sum\limits_{t=1}^{n}y_{t-1}y_{t}}{\sum\limits_{t=1}^{n}y_{t}^{2}}
\]</span></p>
<p>if <span class="math inline">\(\left|\rho \right| &lt; 1\)</span> then</p>
<p><span class="math display">\[
\sqrt{n}\left(\hat{\rho} - \rho\right) \implies \sim{N}(0, 1 - \rho^{2})
\]</span></p>
<p>To compute the test statistics, we fit the augmented Dickey–Fuller regression <span class="math display">\[
\Delta y_{t} = \alpha + \beta y_{t-1} + \delta + \sum\limits_{j=1}^{k}\psi_{i}\Delta y_{t- i} + \epsilon_{t}
\]</span></p>
<p>Depending on the specifications, the constant term <span class="math inline">\(\alpha\)</span> or time trend <span class="math inline">\(\delta\)</span> is omitted and <span class="math inline">\(k\)</span> is the number of lags specified.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
