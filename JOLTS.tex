% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage[]{nimbusserif}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={JOLTS Overview},
  colorlinks=true,
  linkcolor=blue,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[width = 135mm, top = 10mm, bottom = 20mm]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{aea}

\title{JOLTS Overview}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The Job Openings and Labor Turnover Survey (JOLTS) tells us how many job
openings there are each month, how many workers were hired, how many
quit their job, how many were laid off, and how many experienced other
separations (which includes worker deaths).

The JOLTS survey design is a stratified random sample of 20,700 nonfarm
business and government establishments. The sample is stratified by
ownership, region, industry sector, and establishment size class. The
establishments are drawn from a universe of over 9.4 million
establishments compiled by the Quarterly Census of Employment and Wages
(QCEW) program which includes all employers subject to state
unemployment insurance laws and federal agencies subject to the
Unemployment Compensation for Federal Employees program.

Employment estimates are benchmarked, or ratio adjusted, monthly to the
strike-adjusted employment estimates of the Current Employment
Statistics (CES) survey. A ratio of CES to JOLTS employment is used to
adjust the levels for all other JOLTS data elements.

JOLTS data provide information on all pieces that go into the net change
in the number of jobs. These components include hires, layoffs,
voluntary quits, and other job separations (which includes retirements
and worker deaths). Putting those components together reveals the
overall (or net) change. JOLTS data provide information about the end of
one month to the end of the next, whereas the monthly employment numbers
provide information from the middle of one month to the middle of the
next.

The JOLTS estimates also are affected by nonsampling error. Nonsampling
error can occur for many reasons including: the failure to include a
segment of the population; the inability to obtain data from all units
in the sample; the inability or unwillingness of respondents to provide
data on a timely basis; mistakes made by respondents; errors made in the
collection or processing of the data; and errors from the employment
benchmark data used in estimation.\footnote{Nonsampling error occurs
  when a sample is surveyed rather than the entire population. Which
  means that there is a chance that the sample estimates may differ from
  the true population values they represent. The difference, or sampling
  error, varies depending on the particular sample selected. This
  variability is measured by the standard error of the estimate. BLS
  analysis is generally conducted at the 90-percent level of confidence.
  That means that there is a 90-percent chance, or level of confidence,
  that an estimate based on a sample will differ by no more than 1.6
  standard errors from the true population value because of sampling
  error. Sampling error estimates are available at the BLS'
  \href{https://www.bls.gov/jlt/jolts_median_standard_errors.htm}{website}}

\hypertarget{data}{%
\subsection{Data}\label{data}}

One of the more useful indicators is the job seekers ratio, that is, the
ratio of unemployed workers to job openings. On average, there were 10.7
million unemployed workers while there were only 6.5 million job
openings. This translates into a job seeker ratio of about 1.6
unemployed workers to every job opening. Another way to think about
this: for every 16 workers who were officially counted as unemployed,
there were only available jobs for 10 of them.

\begin{center}\includegraphics{JOLTS_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{center}\includegraphics{JOLTS_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{center}\includegraphics{JOLTS_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{center}\includegraphics{JOLTS_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{center}\includegraphics{JOLTS_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{center}\includegraphics{JOLTS_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

We produce forecasts using a collection of traditional and
non-traditional time series methods. This section provides a general
overview of the methods used, their benefits, and their limitations. It
cannot be emphasized enough that, no matter the strength of a model, it
remains exactly that: a model. As such, all statistical models are
``wrong.'' No matter the method used, any model is an attempt to
reproduce (``model'') the true data generating process of a data series.

\hypertarget{background}{%
\subsection{Background}\label{background}}

For many familiar with the methodology of time series forecasting, this
section presents a brief recapitulation of basic concepts which can be
found in any standard textbook such as \cite{MJK}.

Time series analysis is the procedure of using known data values to fit
a time series with a suitable model and estimation of the corresponding
parameters. It comprises methods that attempt to understand the nature
of the time series.

A major assumption that often provides relief in modeling efforts is the
linearity assumption. A linear filter, for example, is a linear
operation from one time series \(x_{t}\) to another time series
\(y_{t}\).

\[
E[y_{t}] = L(x_{t}) = A(L^{i})\epsilon_{t} = \left(\sum\limits_{-\infty}^{\infty} \psi_{i}L^{i}\right)\epsilon_{t} = \mu + \sum\limits_{-\infty}^{\infty}\psi_{i}\epsilon_{t-i},
\]

where \(\epsilon \sim \mathcal{N}(0, \sigma^{2})\) and \(L^{i}\) is the
Lag or Backshift Operator defined as,

\[
L^{i}x_{t} = x_{t-i} \; \forall \, i \in \mathbb{N}
\]

the linear filter can be seen as a process that converts the input,
\(x_{t}\), into an output, \(y_{t}\) with a conversion process that
involves all (present, past, and future) values of the input in the form
of a summation with different ``weights'', \(\psi_{i}\), on each
observation \(x_{i}\). Specifically, \(y_{t}\) can be expressed as

\[
y_{t} = \mu + \sum\limits_{i= o}^{\infty}\psi_{i}\epsilon_{t-i}
\]

where the weights are conventionally some form of moving average \(M\).
A moving average \(M\) of span \(N\) assigns weights \(\frac{1}{N}\) to
the most recent observations such that the estimate can be written as

\[
M_{t} = \frac{1}{N} \sum\limits_{t = T - N - 1}^{N} y_{t}
\]

The covariance between \(y_{t}\) and its value at another point in time
\(y_{t + k}\) is called the auto-covariance at lag \(k\), and is defined
by

\[
\gamma_{k} = \mathrm{Cov}(y_{t}, y_{t+k}) = E\left[(y_{t} - \mu)(y_{t+k} - \mu)\right]
\]

the autocovariance of the series at lag \(k = 0\) is simply the variance
of the time series. The autocorrelation coefficient at \(k\) is then

\begin{gather*}
\rho_{k} = \frac{E[(y_{t} - \mu)(y_{t+k} -\mu)]}{\sqrt{E[(y_{t} - \mu)^{2}]E[(y_{t+k} - \mu)^{2}]}} \\[8pt]
= \frac{\mathrm{Cov}(y_{t}, y_{t+k})}{\mathrm{Var}(y_{t})}\\[8pt]
= \frac{\gamma_{k}}{\gamma_{0}}
\end{gather*}

The collection of these values \(\rho_{k}\) is called the
autocorrelation function. For a finite time series, it is necessary to
estimate the autocovariance and autocorrelation functions. A usual
estimate of the autorcovariance function is

\[
c_{k} = \hat{\gamma_{k}} = \frac{1}{T} \sum\limits_{t = 1}^{T-k}(y_{t} -\bar{y})(y_{t+k} - \bar{y}), \quad k = 1, 2, 3,\, \ldots,\, K
\] and the autocorrelation function is estimated by the sample
autocorrelation function (sample ACF) \[
r_{k} = \hat{\rho_{k}} = \frac{c_{k}}{c_{o}}
\] The stationarity of a time series is related to its statistical
properties in time. That is, in the more strict sense, a stationary time
series exhibits similar ``statistical behavior'' in time and this is
often characterized as a constant probability distribution in time.
However, it is usually satisfactory to consider the first two moments of
the time series and define stationarity (or weak stationarity) as
follows: (1) the expected value of the time series does not depend on
time and (2) the autocovariance function defined as
\(\mathrm{Cov}(y_{t}, y_{t+k})\) for any lag \(k\) is only a function of
\(k\) and not time: that is, \(y(k) = \mathrm{Cov}(y_{t}, y_{t+k})\).
Weak stationarity, then, implies that the mean of the series does not
change with time \(t\) and that the autocovariance function,
\(\gamma(k, t)\), depends on \(k\) and \(t\) only through their
difference \(k - t\).

For a time-invariant and stable linear filter a stationary input time
series \(y(t)\) can be written more succinctly as,

\begin{gather*}
y(t) = \mu + \sum\limits_{i= 0}^{\infty}\psi_{i}\epsilon_{t} \\[8pt]
\mu + \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}\epsilon_{t} \\[8pt]
\mu + \Psi(L)\epsilon_{t}
\end{gather*}

where \[
  \Psi(L)\epsilon_{t} = \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}
  \]\\
The infinite moving average model has a covariance function in the form,

\[
\mathrm{Cov}(y_{t}, y_{t+k}) = \gamma_{y}(k, t) = \sum\limits_{i = -\infty}^{\infty}\sum\limits_{j = - \infty}^{\infty}\psi_{i}\psi_{j}\gamma_{x}(i - j + k)
\]

The autocovariance function of \(y_{t}\) is then,

\begin{gather*}
\gamma_{y}(k, t) = \sum\limits_{i = o}^{\infty}\sum^{\infty}\limits_{j = 0} \psi_{i}\psi_{j}\gamma_{\epsilon}(i-j+k) \\[8pt]
= \sigma^{2}\sum\limits_{i = 0}^{\infty}\psi_{i}\psi_{i + k}
\end{gather*}

This representation comes from a theorem by Wold and which essentially
states that any nondeterministic weakly stationary time series \(y_{t}\)
can be represented as in the above manner. A more intuitive
interpretation of this theorem is that a stationary time series can be
seen as the weighted sum of the present and past random
``disturbances.''

Although very powerful in providing a general representation of any
stationary time series, the infinite moving average model given in is
not very useful in practice except for certain special cases.

In finite order moving average or MA models, weights that are not set to
0 are represented by the Greek letter \(\theta\) with a minus sign in
front.

\begin{gather*}
y_{t} = \mu + \left(1 - \sum\limits_{i=1}^{q}\theta_{i}L^{i} \right)\epsilon_{i}\\[8pt]
= \mu + \Theta(L)\epsilon_{t}
\end{gather*}

where
\(\Theta(L)\epsilon_{t} = 1 - \sum\limits_{i=1}^{q}\theta_{i}L^{i}\)

An interpretation of the finite order MA processes is that at any given
time, of the infinitely many past disturbances, only a finite number of
the disturbances ``contribute'' to the current value of the time series
and that the time window of the contributors ``moves'' in time, making
the ``oldest'' disturbance obsolete for the next observation. It is
indeed not too far fetched to think that some processes might have these
intrinsic dynamics. However, for some others, we may be required to
consider the ``lingering'' contributions of the disturbances that
happened back in the past. This will of course bring us back to square
one in terms of our efforts in estimating infinitely many weights.
Another solution to this problem is through autoregressive models in
which the infinitely many weights are assumed to follow a distinct
pattern and can be successfully represented with only a handful of
parameters. We shall now consider some special cases of autoregressive
processes.

\hypertarget{arma}{%
\subsection{ARMA}\label{arma}}

Autoregressive models are based on the idea that current value of the
series, \(x_{t}\) can be explained as a linear combination of past
values, \(x_{t-1}, x_{t-2}, \ldots, \, x_{t-p}\), together with some
random error \(\epsilon\). In other words, a series \(x_{t}\) is
linearly dependent upon its past values, the degree to which depends on
the lag order \(p\).

Let us first consider again the time series

\begin{gather*}
y(t) = \mu + \sum\limits_{i= 0}^{\infty}\psi_{i}\epsilon_{t} \\[8pt]
\mu + \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}\epsilon_{t} \\[8pt]
\mu + \Psi(L)\epsilon_{t}
\end{gather*}

One approach to modeling this time series is to assume that the
contributions of the disturbances that are way in the past should be
small compared to the more recent disturbances that the process has
experienced. Since the disturbances are independently and identically
distributed random variables, we can simply assume a set of infinitely
many weights in descending magnitudes reflecting the diminishing
magnitudes of contributions of the disturbances in the past.

A simple and yet intuitive set of such weights can be created following
an exponential decay pattern. For that we will set
\(\psi_{i} = \phi^{i}\), where \(\left|\,\phi\,\right| < 1\) to
guarantee the exponential ``decay.'' In this notation, the weights on
the disturbances starting from the current disturbance and going back in
past will be \(1, \phi^{2}, \phi^{3},\, \ldots\)

\begin{gather*}
y_{t} = \mu + \epsilon_{t} + \phi\epsilon_{t-1} + \phi^{2}\epsilon_{t-2} + \cdots\\[8pt]
= \mu + \sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i}
\\[8pt]
\end{gather*}

We also have

\begin{gather*}
y_{t-1} = \mu + \epsilon_{t-1} + \phi\epsilon_{t-2} + \phi^{2}\epsilon_{t-3} + \cdots \\[8pt]
= \mu + \phi\left(\sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-1-k} + \epsilon_{t}\right)
\end{gather*}

combining these two equations together we can represent an AR(1) model
as a linear process given by

\begin{gather*}
\mu + \sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i} = \mu + \phi\left(\sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i} + \epsilon_{t}\right) \\[8pt]
\mu + \epsilon_{t} + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t-i} + \phi\mu \\[8pt]
\mu - \phi\mu + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t-i} + \epsilon_{t} \\[8pt]
\delta + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t- i} + \epsilon_{t}
\end{gather*}

where \(\delta = (1 - \phi)\mu\). A more general way of writing an
autoregressive procress is through operator notation as

\begin{gather*}
\epsilon_{t} = \left(1 - \sum\limits_{i=1}^{i}\phi_{i}L^{i} \right)y_{t} \\[8pt]
 = \Phi(L)y_{t}
\end{gather*}

where \(\Phi(L) = 1 - \sum\limits_{i=1}^{p}\phi_{i}L^{i}\)

The above process in is called a first-order autoregressive process,
AR(l), because it can be seen as a regression of \(y_{t}\) on
\(y_{t-1}\) and hence the term autoregressive process.

Autoregressive--moving-average (ARMA) models provide a parsimonious
description of a (weakly) stationary stochastic process in terms of two
polynomials, one for the autoregression (AR) and the second for the
moving average (MA). The general ARMA model was described in the 1951
dissertation of Peter Whittle, Hypothesis Testing in Time Series, which
generalized Wold's autoregressive representation theorem for univariate
stationary processes to multivariate processes, and was popularized in
the 1970 book by George E. P. Box and Gwilym Jenkins Time Series
Analysis: Forecasting and Control.

\begin{gather*}
y_{t} = \delta + \sum\limits_{i=0}^{p}\phi^{i}y_{t- i} + \epsilon_{t} - \sum\limits_{i=1}^{q}\theta_{i}\epsilon_{i-1} \\[8pt]
\Psi(L)y_{t} = \delta + \Theta(L)\epsilon_{t} 
\end{gather*}

which has an infinite moving average process \(\mathrm{MA}(q)\)

\[
y_{t} = \mu + \sum\limits_{i= o}^{\infty}\psi_{i}\epsilon_{t-i}
\]

\hypertarget{arima}{%
\subsection{ARIMA}\label{arima}}

The Autoregressive Integrated Moving Average (ARIMA) model is a
generalization of an Autoregressive Moving Average (ARMA) model to
overcome the possible violation of the assumption that the series
\(x_{t}\) is stationary. In order for inferences drawn from the
estimates to be reliable when the data are not stationary, the data must
be differenced some number of times \(d\) to remove seasonality and
trend. If a stochastic process has to be differenced \(d\) times to
reach stationarity, it is said to be integrated of order \(d\) or
\(I(d)\).

A standard convention for expressing the ARIMA model is
\(\mathrm{ARIMA}(p, d, q)\), where:

\begin{itemize}
\item
  \(p\) denotes the number of lag observations included in the model,
  also called the lag order.
\item
  \(d\) denotes the number of times that the series is differenced, also
  called the degree of differencing.
\item
  \(q\) denotes the size of the moving average window.
\end{itemize}

We will call a time series homogeneous, non-stationary if it is not
stationary but its first difference, that is,
\(w_{t} = y_{t} - Y_{t-1} =(1 - L)y_{t}\),or higher-order differences,
\(w_{t} =(I- L)^{d}y_{t}\) produce a stationary time series. We will
further call \(y_{t}\) an autoregressive integrated moving average
(\(\mathrm{ARIMA}\)) process of orders \(p\), \(d\), and \(q\)
\(\mathrm{ARIMA}(p, d, q)\) if its \(d\)-th difference, denoted by
\(w_{t} =(I- L)^{d}y_{t}\) produces a stationary \(\mathrm{ARMA}(p, q)\)
process. The term integrated is used since, \(d = 1\), for example, we
can write \(y_{t}\) as the sum (or ``integral'') of the \(w_{t}\)
process as

\begin{gather*}
y\_{t} = \delta + \sum\limits_{i=0}^{p}\phi^{i}y_{t- i} + \epsilon_{t} - \sum\limits_{i=1}^{q}\theta_{i}\epsilon_{i-1} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right)y_{t} = \left(1 + \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\epsilon_{t} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right) = \left(1 + \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\left(1 - L\right)^{d} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right)\left(1 - L\right)^{d}y_{t} = \left(1 + \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\epsilon_{t} \\[8pt]
\Phi(L)(1 - L)^{d}y_{t} = \delta + \Theta(L)\epsilon_{t}
\end{gather*}

Selection of the hyperparameters \((p, d, q)\) are often chosen through
an inspection of the ACF and PACF, but can also be selected through AIC
(Akaike Information Criterion), AICc (corrected AIC) and BIC (Bayesian
Information Criterion). But note that the selection of the
hyperparameters is not unique.

\hypertarget{svm}{%
\subsection{SVM}\label{svm}}

\hypertarget{rnn}{%
\subsection{RNN}\label{rnn}}

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

\hypertarget{unit-root}{%
\subsection{Unit Root}\label{unit-root}}

A unit root is a feature of some stochastic processes (such as random
walks) that can cause problems in statistical inference involving time
series models. A linear stochastic process has a unit root if 1 is a
root of the process's characteristic equation. Shocks to a unit root
process have permanent effects which do not decay as they would if the
process were stationary. The characteristic roots (roots of the
characteristic equation) also provide qualitative information about the
behavior of the variable whose evolution is described by the dynamic
equation. For a differential equation parameterized on time, the
variable's evolution is stable if and only if the real part of each root
is negative. For difference equations, such as a standard time series,
there is stability if and only if the absolute value of each root is
less than 1.

An augmented Dickey--Fuller test (ADF) tests the null hypothesis that a
unit root is present in a time series sample. The alternative hypothesis
is different depending on which version of the test is used, but is
usually stationarity or trend-stationarity.

The intuition behind the test is that if the series is characterised by
a unit root process then the lagged level of the series (\(y_{t-1}\))
will provide no relevant information in predicting the change in
\(y_{t}\) besides the one obtained in the lagged changes. The OLS
estimate (based on an \(n\)-observation time series) of the
autocorrelation parameter \(\rho\) is given by

\[
\hat{\rho}(n) = \frac{\sum\limits_{t=1}^{n}y_{t-1}y_{t}}{\sum\limits_{t=1}^{n}y_{t}^{2}}
\]

if \(\left|\rho \right| < 1\) then

\[
\sqrt{n}\left(\hat{\rho} - \rho\right) \implies \sim{N}(0, 1 - \rho^{2})
\]

To compute the test statistics, we fit the augmented Dickey--Fuller
regression \[
\Delta y_{t} = \alpha + \beta y_{t-1} + \delta + \sum\limits_{j=1}^{k}\psi_{i}\Delta y_{t- i} + \epsilon_{t}
\]

Depending on the specifications, the constant term \(\alpha\) or time
trend \(\delta\) is omitted and \(k\) is the number of lags specified.

  \bibliography{references.bib}

\end{document}
