---
title: "JOLTS Overview"
fontfamily: libertine
fontfamilyoptions: sfdefault
output: 
  pdf_document:
  fig_width: 6
  fig_height: 5
  fig_caption: true
  extra_dependencies: ["amsmath", "subfig", "relsize", "amsfonts"]
fontsize: 11pt
---

<style>
body {
text-align: justify}
</style>

  ```{r include = FALSE}
library(knitr)
library(ggplot2)
library(tinytex)
library(ggthemes)
library(extrafont)
library(boot)
library(ggthemr)
library(tidyverse)
library(dplyr)
library(scales)
library(tidyr)
library(blscrapeR)
library(purrr)
library(lubridate)
library(zoo)
library(fredr)
```

```{r include = FALSE}
raspberry <- "#DB2955"
babyblue <- "#47AFFF"
prussian <- "#113255"
sapphire <- "#255F85"
ggthemr('fresh')
```

  ```{r include = FALSE}
  # FRED API Key
fredr_set_key("5feffa6d832eea67340b601a7e183d01")

# get and wrangle data
reformat <- 
  function(df){
    pivot_wider(df, names_from = series_id, values_from = value) %>%
      rename(openings = JTSJOL, 
             hires = JTSHIL,
             separations = JTSTSL,
             quits = JTSQUL,
             layoffs = JTSLDL,
             unemployed = UNEMPLOY,
             employed = PAYEMS) %>%
      mutate(date = ymd(date)) %>%
      filter(date >= "2001-01-01") %>%
      relocate(date)
  }

df <-
  map_dfr(c( "PAYEMS", "UNEMPLOY", "JTSHIL", "JTSTSL", "JTSJOL", "JTSQUL", "JTSLDL"), fredr) %>%
  reformat()
  ``
#Introduction

The Job Openings and Labor Turnover Survey (JOLTS) tells us how many job openings there are each month, how many workers were hired, how many quit their job, how many were laid off, and how many experienced other separations (which includes worker deaths).

The JOLTS survey design is a stratified random sample of 20,700 nonfarm business and government 
establishments. The sample is stratified by ownership, region, industry sector, and establishment size class. The  establishments are drawn from a universe of over 9.4 million establishments compiled by the Quarterly Census of Employment and Wages (QCEW) program which includes all employers subject to state unemployment insurance laws and federal agencies subject to the Unemployment Compensation for Federal Employees program.

Employment estimates are benchmarked, or ratio adjusted, monthly to the strike-adjusted employment estimates of the Current Employment Statistics (CES) survey. A ratio of CES to JOLTS employment is used to adjust the levels for all other JOLTS data elements.

JOLTS data provide information on all pieces that go into the net change in the number of jobs. These components include hires, layoffs, voluntary quits, and other job separations (which includes retirements and worker deaths). Putting those components together reveals the overall (or net) change. JOLTS data provide information about the end of one month to the end of the next, whereas the monthly employment numbers provide information from the middle of one month to the middle of the next.

JOLTS estimates are subject to both sampling and nonsampling error. Nonsampling error occurs when a sample 
is surveyed rather than the entire population. There is a chance that the sample estimates may differ from the true 
population values they represent. The difference, or sampling error, varies depending on the particular sample 
selected. This variability is measured by the standard error of the estimate. BLS analysis is generally conducted at the 90-percent level of confidence. That means that there is a 90-percent chance, or level of confidence, that an estimate based on a sample will differ by no more than 1.6 standard errors from the true population value because of  sampling error. Sampling error estimates are available at www.bls.gov/jlt/jolts_median_standard_errors.htm.

The JOLTS estimates also are affected by nonsampling error. Nonsampling error can occur for many reasons 
including: the failure to include a segment of the population; the inability to obtain data from all units in the sample; the inability or unwillingness of respondents to provide data on a timely basis; mistakes made by respondents; errors made in the collection or processing of the data; and errors from the employment benchmark data used in estimation.

## Data

One of the most striking indicators from today’s report is the job seekers ratio, that is, the ratio of unemployed workers (averaged for mid-November and mid-December) to job openings (at the end of November). On average, there were 10.7 million unemployed workers while there were only 6.5 million job openings. This translates into a job seeker ratio of about 1.6 unemployed workers to every job opening. Another way to think about this: for every 16 workers who were officially counted as unemployed, there were only available jobs for 10 of them.

```{r dpi = 600,fig.align="center", echo=FALSE, warning = FALSE, message=FALSE}
  df %>%
  ggplot(aes(x = date, y = unemployed/openings)) +
  geom_line(color = prussian) + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom",
        legend.title = element_text(size = 10),
        plot.title = element_text(face = "plain"),
        plot.caption = element_text(hjust = 1, size = 8)) +
  scale_y_continuous(position = "right") +
  theme(text = element_text(color = "black", size = 10)) + 
  labs(y = "Job Seekers Ratio \n", x = "", fill = "", title = "Job Seeker Ratio",
       caption = "Source: Bureau of Labor Statistics: Job Openings and Labor Turnover Survey")
```

```{r dpi = 600,fig.align="center", echo=FALSE, warning = FALSE, message=FALSE}
  df %>%
  ggplot(aes(x = date, y = ((unemployed/openings) - lag((unemployed/openings), n = 12))/lag((unemployed/openings), n = 12))) +
  geom_line(color = prussian) + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom",
        legend.title = element_text(size = 10),
        plot.title = element_text(face = "plain"),
        plot.caption = element_text(hjust = 1, size = 8)) +
  scale_y_continuous(position = "right") +
  theme(text = element_text(color = "black", size = 10)) + 
  labs(y = "Job Seekers Ratio \n", x = "", fill = "", title = "Job Seeker Ratio",
       caption = "Source: Bureau of Labor Statistics: Job Openings and Labor Turnover Survey")
```

```{r dpi = 600, fig.align="center", echo = FALSE, warning = FALSE, message=FALSE}
  df %>%
  mutate(seekers = unemployed/(unemployed + employed),
         vacany = openings/(unemployed + employed)) %>%
  gather(measure, value, seekers, vacany) %>%
  ggplot(aes(x = date, y = value, color = measure)) +
  geom_line() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom",
        legend.title = element_text(size = 10),
    plot.title = element_text(face = "plain"),
    plot.caption = element_text(hjust = 1, size = 8)) +
  scale_y_continuous(label = percent_format(), position = "right") +
  scale_color_manual("", labels = c("Unemployment rate", "Vacancy rate"), values = c(prussian, raspberry)) +
  theme(text = element_text(color = "black", size = 10)) + 
  labs(y = "Percent \n", x = "", fill = "", title = "Unemployment & Vacany Rate",
       caption = "Source: Bureau of Labor Statistics, Job Openings and Labor Turnover Survey")
```

```{r dpi = 600, fig.align="center", echo = FALSE, warning = FALSE, message=FALSE}
 df %>%
  mutate(Hires = hires/employed,
         Quits = quits/employed,
         Layoffs = layoffs/employed) %>%
  gather(measure, value, Hires, Quits, Layoffs) %>%
    ggplot(aes(x = date, y = value, colour = measure)) +
    geom_line() +
    theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom",
        legend.title = element_text(size = 10),
        plot.title = element_text(face = "plain"),
        plot.caption = element_text(hjust = 1, size = 8)) +
  scale_y_continuous(label = percent_format(), position = "right") +
  scale_color_manual("",values = c(prussian, babyblue, raspberry)) +
  theme(text = element_text(color = "black", size = 10)) + 
  labs(y = "Percent \n", x = "", fill = "", title = "Hires, Quits, & Layoffs",
       caption = "Source: Bureau of Labor Statistics, Job Openings and Labor Turnover Survey")
```

```{r dpi = 600, fig.align="center", echo = FALSE, warning = FALSE, message=FALSE}
 df %>%
  mutate(Hires = rollmean((hires - lag(hires, n = 12, fill = NA))/lag(hires, n = 12), k = 12, fill = NA),
         Quits = rollmean((quits - lag(quits, n = 12, fill = NA))/lag(quits, n = 12), k = 12, fill = NA),
         Layoffs = rollmean((layoffs - lag(layoffs, n = 12, fill = NA))/lag(layoffs, n = 12), k = 12, fill = NA)) %>%
  gather(measure, value, Hires, Quits, Layoffs) %>%
  ggplot(aes(x = date, y = value, colour = measure)) +
  geom_line() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom",
        legend.title = element_text(size = 10),
        plot.title = element_text(face = "plain"),
        plot.caption = element_text(hjust = 1, size = 8)) +
  scale_y_continuous(label = percent_format(), position = "right") +
  scale_color_manual("",values = c(prussian, babyblue, raspberry)) +
  theme(text = element_text(color = "black", size = 10)) + 
  labs(y = "Percent \n", x = "", fill = "", title = "Hires, Quits, & Layoffs (12-month rolling average)",
       caption = "Source: Bureau of Labor Statistics, Job Openings and Labor Turnover Survey")
```

```{r dpi = 600, fig.align="center", echo = FALSE, warning = FALSE, message=FALSE}
df %>% 
  mutate(Hires = rollmean((hires - lag(hires, n = 12, fill = NA))/lag(hires, n = 12), k = 12, fill = NA)) %>%
  ggplot(aes(x = date, y = Hires)) +
  geom_line(color = prussian) + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom",
        legend.title = element_text(size = 10),
        plot.title = element_text(face = "plain"),
        plot.caption = element_text(hjust = 1, size = 8)) +
  scale_y_continuous(label = percent_format(), position = "right") +
  theme(text = element_text(color = "black", size = 10)) + 
  labs(y = "Percent \n", x = "", fill = "", title = "Hiring Rate (12-month rolling average)",
       caption = "Source: Bureau of Labor Statistics, Job Openings and Labor Turnover Survey")
```

## Methods

We produce forecasts using a collection of traditional and non-traditional time series methods. This section provides a general overview of the methods used, their benefits, and their limitations. It cannot be emphasized enough that, no matter the strength of a model, it remains exactly that:  a model. As such, all statistical models are “wrong.” No matter the method used, any model is an attempt to reproduce (“model”) the true data generating process of a data series.

## Background

Time series analysis is the procedure of using known data values to fit a time series with a suitable model and estimation of  the corresponding parameters. It comprises methods that attempt to understand the nature of the time series.

A major assumption that often provides relief in modeling efforts is the linearity assumption. A linear
filter, for example, is a linear operation from one time series $x_{t}$ to another time series $y_{t}$.

\[
E[y_{t}] = L(x_{t}) = A(L^{i})\epsilon_{t} = \left(\sum\limits_{-\infty}^{\infty} \psi_{i}L^{i}\right)\epsilon_{t} = \mu + \sum\limits_{-\infty}^{\infty}\psi_{i}\epsilon_{t-i},
\]

where $\epsilon \sim \mathcal{N}(0, \sigma^{2})$ and $L^{i}$ is the Lag or Backshift Operator defined as,

\[
L^{i}x_{t} = x_{t-i} \; \forall \, i \in \mathbb{N}
\]

the linear filter can be seen as a process that converts the input, $x_{t}$, into an output, $y_{t}$ with a conversion process that involves all (present, past, and future) values of the input in the form of a summation with different "weights", $\psi_{i}$, on each observation $x_{i}$. Specifically, $y_{t}$ can be expressed as

\[
y_{t} = \mu + \sum\limits_{i= o}^{\infty}\psi_{i}\epsilon_{t-i}
\]

where the weights are conventionally some form of moving average $M$. A moving average $M$ of span $N$ assigns weights $\frac{1}{N}$ to the most recent observations such that the estimate can be written as
\[
M_{t} = \frac{1}{N} \sum\limits_{t = T - N - 1}^{N} y_{t}
\]

The covariance between $y_{t}$ and its value at another point in time $y_{t + k}$ is called the auto-covariance at lag $k$, and is defined by

\[
\gamma_{k} = \mathrm{Cov}(y_{t}, y_{t+k}) = E\left[(y_{t} - \mu)(y_{t+k} - \mu)\right]
\]

the autocovariance of the series at lag $k = 0$ is simply the variance of the time series. The autocorrelation coefficient at $k$ is then
\begin{gather*}
\rho_{k} = \frac{E[(y_{t} - \mu)(y_{t+k} - \mu)]}{\sqrt{E[(y_{t} - \mu)^{2}]E[(y_{t+k} - \mu)^{2}]}} \\[8pt]
= \frac{\mathrm{Cov}(y_{t}, y_{t+k})}{\mathrm{Var}(y_{t})}\\[8pt]
= \frac{\gamma_{k}}{\gamma_{0}}
\end{gather*}

The collection of these values $\rho_{k}$ is called the autocorrelation function. For a finite time series, it is necessary to estimate the autocovariance and autocorrelation functions. A usual estimate of the autorcovariance function is

\[
c_{k} = \hat{\gamma_{k}} = \frac{1}{T} \sum\limits_{t = 1}^{T-k}(y_{t} -\bar{y})(y_{t+k} - \bar{y}), \quad k = 1, 2, 3,\, \ldots,\, K
\]

and the autocorrelation function is estimated by the sample autocorrelation function (sample ACF)

\[
r_{k} = \hat{\rho_{k}} = \frac{c_{k}}{c_{o}}
\]

The stationarity of a time series is related to its statistical properties in time. That is, in the more strict sense, a stationary time series exhibits similar "statistical behavior" in time and this is often characterized as a constant probability distribution in time. However, it is usually satisfactory to consider the first two moments of the time series and define stationarity (or weak stationarity) as follows: (1) the expected value of the time series does not depend on time and (2) the autocovariance function defined as $\mathrm{Cov}(y_{t}, y_{t+k})$ for any lag $k$ is only a function of $k$ and not time: that is, $y(k) = \mathrm{Cov}(y_{t}, y_{t+k})$. Weak stationarity, then, implies that the mean of the series does not change with time $t$ and that the autocovariance function, $\gamma(k, t)$, depends on $k$ and $t$ only
through their difference $|k - t|$.

For a time-invariant and stable linear filter a stationary input time series $y(t)$ can be written more succinctly as,

\begin{gather*}
y(t) = \mu + \sum\limits_{i= 0}^{\infty}\psi_{i}\epsilon_{t} \\[8pt]
\mu + \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}\epsilon_{t} \\[8pt]
\mu + \Psi(L)\epsilon_{t}
\end{gather*}

where $\Psi(L)\epsilon_{t} = \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}$.

The infinite moving average model has a covariance function in the form,

\[
\mathrm{Cov}(y_{t}, y_{t+k}) = \gamma_{y}(k, t) = \sum\limits_{i = -\infty}^{\infty}\sum\limits_{j = - \infty}^{\infty}\psi_{i}\psi_{j}\gamma_{x}(i - j + k)
\]

The autocovariance function of $y_{t}$ is then,

\begin{gather*}
\gamma_{y}(k, t) = \sum\limits_{i = o}^{\infty}\sum^{\infty}\limits_{j = 0} \psi_{i}\psi_{j}\gamma_{\epsilon}(i-j+k) \\[8pt]
= \sigma^{2}\sum\limits_{i = 0}^{\infty}\psi_{i}\psi_{i + k}
\end{gather*}

This representation comes from a theorem by Wold and which essentially states that any nondeterministic weakly stationary time series $y_{t}$ can be represented as in the above manner. A more intuitive interpretation of this theorem is that a stationary time series can be seen as the weighted sum of the present and past random "disturbances."

Although very powerful in providing a general representation of any stationary time series, the infinite moving average model given in is not very useful in practice except for certain special cases.

In finite order moving average or MA models, weights that are not set to 0 are represented by the Greek letter $\theta$ with a minus sign in front.

\begin{gather*}
y_{t} = \mu + \left(1 - \sum\limits_{i=1}^{q}\theta_{i}L^{i} \right)\epsilon_{i}\\[8pt]
= \mu + \Theta(L)\epsilon_{t}
\end{gather*}

where $\Theta(L)\epsilon_{t} = 1 - \sum\limits_{i=1}^{q}\theta_{i}L^{i}$

An interpretation of the finite order MA processes is that at any given time, of the infinitely many past disturbances, only a finite number of the disturbances "contribute" to the current value of the time series and that the time window of the contributors "moves" in time, making the "oldest" disturbance obsolete for the next observation. It is indeed not too far fetched to think that some processes might have these intrinsic dynamics. However, for some others, we may be required to consider the "lingering" contributions of the disturbances that happened back in the past. This will of course bring us back to square one in terms of our efforts in estimating infinitely many weights. Another solution to this problem is through autoregressive models in which the infinitely many weights are assumed to follow a distinct pattern and can be successfully represented with only a handful of parameters. We shall now consider some special cases of autoregressive processes.

## ARMA

Autoregressive models are based on the idea that current value of the series, $x_{t}$ can be explained as a linear combination of past values, $x_{t-1}, x_{t-2}, \ldots, \, x_{t-p}$, together with some random error $\epsilon$. In other words, a series $x_{t}$ is linearly  dependent upon its past values, the degree to which depends on the lag order $p$.  

Let us first consider again the time series

\begin{gather*}
y(t) = \mu + \sum\limits_{i= 0}^{\infty}\psi_{i}\epsilon_{t} \\[8pt]
\mu + \sum\limits_{i=0}^{\infty}\psi_{i}L^{i}\epsilon_{t} \\[8pt]
\mu + \Psi(L)\epsilon_{t}
\end{gather*}

One approach to modeling this time series is to assume that the contributions of the disturbances that are way in the past should be small compared to the more recent disturbances that the process has experienced. Since the disturbances are independently and identically distributed random variables, we can simply assume a set of infinitely many weights in descending magnitudes reflecting the diminishing magnitudes of contributions of the disturbances in the past.

A simple and yet intuitive set of such weights can be created following an exponential decay pattern. For that we will set $\psi_{i}  = \phi^{i}$, where $\left|\,\phi\,\right| < 1$ to guarantee the exponential "decay." In this notation, the weights on the disturbances starting from the current disturbance and going back in past will be $1, \phi^{2}, \phi^{3},\, \ldots$

\begin{gather*}
y_{t} = \mu + \epsilon_{t} + \phi\epsilon_{t-1} + \phi^{2}\epsilon_{t-2} + \cdots\\[8pt]
= \mu + \sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i}
\\[8pt]
\end{gather*}

We also have

\begin{gather*}
y_{t-1} = \mu + \epsilon_{t-1} + \phi\epsilon_{t-2} + \phi^{2}\epsilon_{t-3} + \cdots \\[8pt]
= \mu + \phi\left(\sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-1-k} + \epsilon_{t}\right)
\end{gather*}

combining these two equations together we can represent an AR(1) model as a linear process given by

\begin{gather*}
\mu + \sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i} = \mu + \phi\left(\sum\limits_{i=0}^{\infty}\phi^{i}\epsilon_{t-i} + \epsilon_{t}\right) \\[8pt]
\mu + \epsilon_{t} + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t-i} + \phi\mu \\[8pt]
\mu - \phi\mu + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t-i} + \epsilon_{t} \\[8pt]
\delta + \sum\limits_{i=0}^{\infty}\phi^{i}y_{t- i} + \epsilon_{t}
\end{gather*}

where $\delta = (1 - \phi)\mu$. A more general way of writing an autoregressive procress is through operator notation as

\begin{gather*}
\epsilon_{t} = \left(1 - \sum\limits_{i=1}^{i}\phi_{i}L^{i} \right)y_{t} \\[8pt]
 = \Phi(L)y_{t}
\end{gather*}

where $\Phi(L) = 1 - \sum\limits_{i=1}^{p}\phi_{i}L^{i}$

The above process in is called a first-order autoregressive process, AR(l), because it can be seen as a regression of $y_{t}$ on $y_{t-1}$ and hence the term autoregressive process.

Autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA). The general ARMA model was described in the 1951 dissertation of Peter Whittle, Hypothesis Testing in Time Series, which generalized Wold's autoregressive representation theorem for univariate stationary processes to multivariate processes, and was popularized in the 1970 book by George E. P. Box and Gwilym Jenkins Time Series Analysis: Forecasting and Control.

\begin{gather*}
y_{t} = \delta + \sum\limits_{i=0}^{p}\phi^{i}y_{t- i} + \epsilon_{t} - \sum\limits_{i=1}^{q}\theta_{i}\epsilon_{i-1} \\[8pt]
\Psi(L)y_{t} = \delta + \Theta(L)\epsilon_{t}
\end{gather*}
which has an infinite moving average process $\mathrm{MA}(q)$
\[
y_{t} = \mu + \sum\limits_{i= o}^{\infty}\psi_{i}\epsilon_{t-i}
\]

## ARIMA

The Autoregressive Integrated Moving Average (ARIMA) model is a generalization of an Autoregressive Moving Average (ARMA) model to overcome the possible violation of the assumption that the series $x_{t}$ is stationary. In order for inferences drawn from the estimates to be reliable when the data are not stationary, the data must be differenced some number of times $d$ to remove seasonality and trend. If a stochastic process has to be differenced $d$ times to reach stationarity, it is said to be integrated of order $d$ or $I(d)$.

A standard convention for expressing the ARIMA model is $\mathrm{ARIMA}(p, d, q)$, where:

* $p$ denotes the number of lag observations included in the model, also called the lag order.

* $d$ denotes the number of times that the series is differenced, also called the degree of differencing.

* $q$ denotes the size of the moving average window.

We will call a time series homogeneous, non-stationary if it is not stationary but its first difference, that is, $w_{t} = y_{t} - Y_{t-1} =(1 - L)y_{t}$,or higher-order differences, $w_{t} =(I- L)^{d}y_{t}$ produce a stationary time series. We will further call $y_{t}$ an autoregressive integrated moving average ($\mathrm{ARIMA}$) process of orders $p$, $d$, and $q$ $\mathrm{ARIMA}(p, d, q)$ if its $d$-th difference, denoted by $w_{t} =(I- L)^{d}y_{t}$ produces a stationary $\mathrm{ARMA}(p, q)$ process. The term integrated is used since, $d = 1$, for example, we can write $y_{t}$ as the sum (or "integral") of the $w_{t}$ process as
\begin{gather*}
y_{t} = \delta + \sum\limits_{i=0}^{p}\phi^{i}y_{t- i} + \epsilon_{t} - \sum\limits_{i=1}^{q}\theta_{i}\epsilon_{i-1} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right)y_{t} = \left(1 +  \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\epsilon_{t} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right) = \left(1 +  \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\left(1 - L\right)^{d} \\[8pt]
\left(1 - \sum\limits_{i=0}^{p}\phi^{i}L^{i}\right)\left(1 - L\right)^{d}y_{t} = \left(1 +  \sum\limits_{i=1}^{q}\theta_{i}L^{i}\right)\epsilon_{t} \\[8pt]
\Phi(L)(1 - L)^{d}y_{t} = \delta + \Theta(L)\epsilon_{t}
\end{gather*}

Selection of the hyperparameters $(p, d, q)$ are often chosen through an inspection of the ACF and PACF, but can also be selected through AIC (Akaike Information Criterion), AICc (corrected AIC) and BIC (Bayesian Information Criterion). But note that the selection of the hyperparameters is not unique.

## SVM

## RNN

## Random Forest

## Unit Root

A unit root is a feature of some stochastic processes (such as random walks) that can cause problems in statistical inference involving time series models. A linear stochastic process has a unit root if 1 is a root of the process's characteristic equation. Shocks to a unit root process have permanent effects which do not decay as they would if the process were stationary. The characteristic roots (roots of the characteristic equation) also provide qualitative information about the behavior of the variable whose evolution is described by the dynamic equation. For a differential equation parameterized on time, the variable's evolution is stable if and only if the real part of each root is negative. For difference equations, such as a standard time series, there is stability if and only if the absolute value of each root is less than 1.

An augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.

The intuition behind the test is that if the series is characterised by a unit root process then the lagged level of the series ($y_{t-1}$) will provide no relevant information in predicting the change in $y_{t}$ besides the one obtained in the lagged changes. The OLS estimate (based on an $n$-observation time series) of the autocorrelation parameter $\rho$ is given by

\[
\hat{\rho}(n) = \frac{\sum\limits_{t=1}^{n}y_{t-1}y_{t}}{\sum\limits_{t=1}^{n}y_{t}^{2}}
 \]
if $\left|\rho \right| < 1$ then

\[
\sqrt{n}\left(\hat{\rho} - \rho\right) \implies \sim{N}(0, 1 - \rho^{2})
\]

To compute the test statistics, we fit the augmented Dickey–Fuller regression
\[
\Delta y_{t} = \alpha + \beta y_{t-1} + \delta + \sum\limits_{j=1}^{k}\psi_{i}\Delta y_{t- i} + \epsilon_{t}
\]

Depending on the specifications, the constant term $\alpha$ or time trend $\delta$ is omitted and $k$ is the number of lags specified.
